{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "N_layers = 3\n",
    "\n",
    "# Number of neurons in each hidden layers\n",
    "N_hidden_neurons = 500\n",
    "N_input_neurons = 784\n",
    "N_output_neurons = 10\n",
    "\n",
    "# Training parameters\n",
    "BETA = 1\n",
    "epsilon = 0.5\n",
    "n_iter1 = 100\n",
    "n_iter2 = 6\n",
    "\n",
    "alpha = np.zeros(N_layers)\n",
    "a_xh1 = 0.4\n",
    "a_h1h2 = 0.1\n",
    "a_h2y = 0.01\n",
    "\n",
    "MINI_BATCH_SIZE = 20\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glorot-Bengio initialization\n",
    "def glorot_bengio_init(n_in, n_out):\n",
    "    std_dev = math.sqrt(2 / (n_in + n_out))\n",
    "    return np.random.normal(0, std_dev, (n_in, n_out))\n",
    "\n",
    "W_xh1 = np.array(glorot_bengio_init(N_input_neurons, N_hidden_neurons))\n",
    "W_h1h2 = np.array(glorot_bengio_init(N_hidden_neurons, N_hidden_neurons))\n",
    "W_h2y = np.array(glorot_bengio_init(N_hidden_neurons, N_output_neurons))\n",
    "\n",
    "B_h1 = np.zeros(N_hidden_neurons)\n",
    "B_h2 = np.zeros(N_hidden_neurons)\n",
    "B_y = np.zeros(N_output_neurons)\n",
    "\n",
    "input_layer = np.zeros(N_input_neurons)\n",
    "hlayer1 = np.zeros(N_hidden_neurons)\n",
    "hlayer2 = np.zeros(N_hidden_neurons)\n",
    "output_layer = np.zeros(N_output_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_sigmoid(x):\n",
    "    a = np.clip(x, 0, 1)\n",
    "    return a\n",
    "\n",
    "def del_hard_sigmoid(x):\n",
    "    a = hard_sigmoid(x)\n",
    "    slope = int(x == a)\n",
    "    return slope\n",
    "\n",
    "def ds_dt(W_prev, layer_prev, W_next, layer_next, s_i, i, b_i, y_hat, is_op, beta):\n",
    "    de_ds = 0\n",
    "    dc_ds = 0\n",
    "    if(is_op):\n",
    "        de_ds += W_prev[i, :] * hard_sigmoid(layer_prev) + b_i\n",
    "        dc_ds = beta * (y_hat - s_i)\n",
    "    else:\n",
    "        de_ds += W_prev[i, :] * hard_sigmoid(layer_prev) + W_next[:, i] * hard_sigmoid(layer_next) + b_i\n",
    "\n",
    "\n",
    "    de_ds *= del_hard_sigmoid(s_i) \n",
    "    de_ds -= s_i\n",
    "\n",
    "    return de_ds + dc_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\IIT\\3rd Year\\5th Sem\\EE746\\Project\\main.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIT/3rd%20Year/5th%20Sem/EE746/Project/main.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIT/3rd%20Year/5th%20Sem/EE746/Project/main.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/IIT/3rd%20Year/5th%20Sem/EE746/Project/main.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m mnist  \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIT/3rd%20Year/5th%20Sem/EE746/Project/main.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Load the MNIST dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/IIT/3rd%20Year/5th%20Sem/EE746/Project/main.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[39m=\u001b[39m mnist\u001b[39m.\u001b[39mload_data()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.datasets import mnist  \n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten the images and reshape to [0, 1]\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "\n",
    "# Perform a test-train split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index = 10\n",
    "img = x_train[index].reshape(28, 28)\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000,)\n",
      "(10000, 784)\n",
      "(10000,)\n",
      "48000\n"
     ]
    }
   ],
   "source": [
    "# print(np.max(x_train[100]))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(len(x_train))\n",
    "# print(x_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "4 4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output_layer)):\n",
    "    print(i+1)\n",
    "    if (i+1) == y_train[10]:\n",
    "        print(i+1, y_train[10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mds_dt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mW_prev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlayer_prev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mW_next\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlayer_next\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0ms_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mb_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mis_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\shambhavi shanker\\appdata\\local\\temp\\ipykernel_20624\\2015541835.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "?ds_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_batches = np.array_split(x_train, len(x_train) // MINI_BATCH_SIZE)\n",
    "print(x_train_batches.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_xh1 = np.zeros(W_xh1.shape)\n",
    "dw_h1h2 = np.zeros(W_h1h2.shape)\n",
    "dw_h2y = np.zeros(W_h2y.shape)\n",
    "\n",
    "db_h1 = np.zeros(B_h1.shape)\n",
    "db_h2 = np.zeros(B_h2.shape)\n",
    "db_y = np.zeros(B_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mini_batch in x_train_batches:\n",
    "\n",
    "    dw_xh1 = np.zeros(W_xh1.shape)\n",
    "    dw_h1h2 = np.zeros(W_h1h2.shape)\n",
    "    dw_h2y = np.zeros(W_h2y.shape)\n",
    "\n",
    "    db_h1 = np.zeros(B_h1.shape)\n",
    "    db_h2 = np.zeros(B_h2.shape)\n",
    "    db_y = np.zeros(B_y.shape)\n",
    "\n",
    "    for idx in range(len(mini_batch)):\n",
    "        # One training example\n",
    "\n",
    "        # Forward Pass \n",
    "        input_layer = mini_batch[idx] # x is clamped\n",
    "\n",
    "        print(\"Number is \", y_train[idx])\n",
    "        \n",
    "        for iter in range(n_iter1):\n",
    "            for i in range(len(hlayer1)):\n",
    "                hlayer1[i] += epsilon * ds_dt(W_xh1, input_layer, W_h1h2, hlayer2, hlayer1[i], i, B_h1[i], None, False, 0)\n",
    "                hlayer1[i] = np.clip(hlayer1[i], 0, 1)\n",
    "            \n",
    "            for i in range(len(hlayer2)):\n",
    "                hlayer2[i] += epsilon * ds_dt(W_h1h2, hlayer1, W_h2y, output_layer, hlayer2[i], i, B_h2[i], None, False, 0)\n",
    "                hlayer1[i] = np.clip(hlayer1[i], 0, 1)\n",
    "\n",
    "            for i in range(len(output_layer)):\n",
    "                y_hat = 0\n",
    "                if (i+1) == y_train[idx]:\n",
    "                    y_hat = 1 \n",
    "                output_layer[i] += epsilon * ds_dt(W_h2y, hlayer2, None, None, output_layer[i], i, B_y[i], y_hat, True, 0)\n",
    "                output_layer[i] = np.clip(output_layer[i], 0, 1)\n",
    "\n",
    "        # print(\"Output layer\", output_layer)\n",
    "\n",
    "        # Collect activations\n",
    "\n",
    "        for i in range(len(hlayer2)):\n",
    "            for j in range(len(output_layer)):\n",
    "                dw_h2y[j, i] -= hard_sigmoid(output_layer[j]) * hard_sigmoid(hlayer2[i]) \n",
    "        \n",
    "        for i in range(len(hlayer1)):\n",
    "            for j in range(len(hlayer2)):\n",
    "                dw_h1h2[j, i] -= hard_sigmoid(hlayer2[j]) * hard_sigmoid(hlayer1[i])\n",
    "\n",
    "        for i in range(len(input_layer)):\n",
    "            for j in range(len(hlayer1)):\n",
    "                dw_xh1[j, i] -= hard_sigmoid(hlayer1[j]) * hard_sigmoid(input_layer[i])\n",
    "\n",
    "        for i in range(len(output_layer)):\n",
    "            db_y[i] -= hard_sigmoid(output_layer[i])\n",
    "        \n",
    "        for i in range(len(hlayer2)):\n",
    "            db_h2[i] -= hard_sigmoid(hlayer2[i])\n",
    "\n",
    "        for i in range(len(hlayer1)):\n",
    "            db_h1[i] -= hard_sigmoid(hlayer1[i])\n",
    "\n",
    "        # Backward Pass\n",
    "        for iter in range(n_iter2):\n",
    "            for i in range(len(output_layer)):\n",
    "                y_hat = 0\n",
    "                if (i+1) == y_train[idx]:\n",
    "                    y_hat = 1\n",
    "                output_layer[i] += epsilon * ds_dt(W_h2y, hlayer2, None, None, output_layer[i], i, B_y[i], y_hat, True, BETA)\n",
    "                output_layer[i] = np.clip(output_layer[i], 0, 1)\n",
    "\n",
    "            for i in range(len(hlayer2)):\n",
    "                hlayer2[i] += epsilon * ds_dt(W_h1h2, hlayer1, W_h2y, output_layer, hlayer2[i], i, B_h2[i], None, False, BETA)\n",
    "                hlayer2[i] = np.clip(hlayer2[i], 0, 1)\n",
    "\n",
    "            for i in range(len(hlayer1)):\n",
    "                hlayer1[i] += epsilon * ds_dt(W_xh1, input_layer, W_h1h2, hlayer2, hlayer1[i], i, B_h1[i], None, False, BETA)\n",
    "                hlayer1[i] = np.clip(hlayer1[i], 0, 1)\n",
    "\n",
    "        # print(\"Output layer\", output_layer)\n",
    "\n",
    "        # Collect activations\n",
    "\n",
    "        for i in range(len(hlayer2)):\n",
    "            for j in range(len(output_layer)):\n",
    "                dw_h2y[j, i] += hard_sigmoid(output_layer[j]) * hard_sigmoid(hlayer2[i]) \n",
    "        \n",
    "        for i in range(len(hlayer1)):\n",
    "            for j in range(len(hlayer2)):\n",
    "                dw_h1h2[j, i] += hard_sigmoid(hlayer2[j]) * hard_sigmoid(hlayer1[i])\n",
    "\n",
    "        for i in range(len(input_layer)):\n",
    "            for j in range(len(hlayer1)):\n",
    "                dw_xh1[j, i] += hard_sigmoid(hlayer1[j]) * hard_sigmoid(input_layer[i])\n",
    "        \n",
    "        for i in range(len(output_layer)):\n",
    "            db_y[i] += hard_sigmoid(output_layer[i])\n",
    "\n",
    "        for i in range(len(hlayer2)):\n",
    "            db_h2[i] += hard_sigmoid(hlayer2[i])\n",
    "\n",
    "        for i in range(len(hlayer1)):\n",
    "            db_h1[i] += hard_sigmoid(hlayer1[i])\n",
    "\n",
    "\n",
    "    # Update weights only after each minibatch\n",
    "    for i in range(len(hlayer2)):\n",
    "        for j in range(len(output_layer)):\n",
    "            W_h2y[j, i] += a_h2y * (dw_h2y[j, i] / MINI_BATCH_SIZE)\n",
    "    \n",
    "    for i in range(len(hlayer1)):\n",
    "        for j in range(len(hlayer2)):\n",
    "            W_h1h2[j, i] += a_h1h2 * (dw_h1h2[j, i] / MINI_BATCH_SIZE)\n",
    "\n",
    "    for i in range(len(input_layer)):\n",
    "        for j in range(len(hlayer1)):\n",
    "            W_xh1[j, i] += a_xh1 * (dw_xh1[j, i] / MINI_BATCH_SIZE)\n",
    "    \n",
    "    for i in range(len(output_layer)):\n",
    "        B_y[i] += a_h2y * (db_y[i] / MINI_BATCH_SIZE)\n",
    "\n",
    "    for i in range(len(hlayer2)):\n",
    "        B_h2[i] += a_h1h2 * (db_h2[i] / MINI_BATCH_SIZE)\n",
    "\n",
    "    for i in range(len(hlayer1)):\n",
    "        B_h1[i] += a_xh1 * (db_h1[i] / MINI_BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_mean_free = T.mean(self.__energy(self.layers))\n",
    "E_mean_weakly_clamped = T.mean(self.__energy(layers_weakly_clamped))\n",
    "biases_dot = T.grad( (E_mean_weakly_clamped-E_mean_free) / beta, self.biases,  consider_constant=layers_weakly_clamped)\n",
    "biases_new  = [b - alpha * dot for b,alpha,dot in zip(self.biases[1:],alphas,biases_dot[1:])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
